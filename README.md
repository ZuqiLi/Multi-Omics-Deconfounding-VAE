# Multi-omics Deconfounding Variational Autoencoder

![fig3-02](https://github.com/user-attachments/assets/9eba9db0-bd98-4fca-8889-ca44b93121e5)

MODVAE (**M**ulti-**O**mics **D**econfounding **V**ariational **A**uto**E**coder) is a project proposing four novel VAE-based deconfounding frameworks tailored for clustering multi-omics data. These frameworks effectively mitigate confounding effects while preserving genuine biological patterns. The deconfounding strategies employed include (A) a conditional VAE, (B) adversarial training, (C) adding a regularization term to the loss function, and (D) removal of latent features correlated with confounders. For all models, we use a X-shaped architecture to merge the heterogeneous input data sources into a combined latent representation [1]. Consensus clustering is applied to the latent representation generated by each VAE-based model.
Details of this project can be found in our publication [2].

## Environment
For better reproducibility, it's recommended to refer to the following hardware and software settings:
- Operating system: Ubuntu 20.04.6 LTS
- Processor: Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz
- Memory: 767 GiB
- Graphics: llvmpipe (LLVM 12.0.0, 256 bits)
- Python version: 3.9.7

The required packages can be installed with the conda environment file in this repository:
```
## cd Multi-Omics-Deconfounding-VAE
conda env create -f environment.yml
source activate env_multiviewVAE
```

## Overview models:

- all models are in folder: `models`
- optimal architecture from `modelOptimisation` experiments: `latentSize = 50`; `hiddenlayers = 200`

- **XVAE**: 
    - `XVAE` - Simidjievski, Nikola, et al. [1]

<br/><img src="https://user-images.githubusercontent.com/7692477/233080494-22abb000-8def-4ddb-b9a2-fa2a582392d2.png" width="300">

- **cXVAE**:
    1. `cXVAE.cXVAE_inputEmbed`: condition confounders on both the input and embedding layers
    2. `cXVAE.cXVAE_input`: condition confounders on the input layer
    3. `cXVAE.cXVAE_embed`: condition confounders on the embedding layer
    4. `cXVAE.cXVAE_fusedEmbed`: condition confounders on both the fusion and the embedding layers

- **Adversarial Training**
    1. XVAE with one adversarial network and multiclass predicition: `adversarial_XVAE_multiclass`
        - `XVAE_adversarial_multiclass`: inspired by Dincer et al. [3]; training over all batches
        - `XVAE_adversarial_1batch_multiclass`: original by Dincer et al. [3] \
![m_btaa796f3](https://github.com/user-attachments/assets/a287b681-757b-4289-bc11-cbe6ce79522c)
        - `XVAE_scGAN_multiclass`: inspired by Bahrami et al. [4] \
![m_btaa976f1](https://github.com/user-attachments/assets/be639762-6dcf-41be-8767-0bc217735527)

    2. XVAE with multiple adversarial network (one for each confounder): `adversarial_XVAE_multipleAdvNet`
 
- **Regularization to loss function**
    1. `XVAE_corrReg`: inspired by Liu et al. [5]
    2. Possibile regularization functions
       1. 'corrAbs': absolute Pearson correlation
       2. 'corrSq': squared Pearson correlation
       3. 'MIhist': mutual information implemented with histogram
       4. 'MIkde': mutual information implemented with KDE
 
- **Removal of latent features**
    1. that are correlated with confounders based on Pearson correlation
    2. that are statistically associated with confounders based on P-value

## Tutorial
To use our MODVAE models, please download/clone the whole repository from Github to your local. \
For the best-performing model, cXVAE, we've created a wrapper file `cXVAE_wrapper.py` for easy usage. It can be run via the following command on terminal:
```
## cd Multi-Omics-Deconfounding-VAE
python  cXVAE_wrapper.py  pathname  filename_view1  filename_view2  filename_label  conf_type  filename_conf
```
### Parameters:
- `pathname`: The full path to the local repository.
- `filename_view1`: The filename of the first data view. It must be a comma-separated (csv) file with $n$ rows (samples) and $p$ columns (features), without rownames or colnames.
- `filename_view2`: The filename of the second data view. It must be a comma-separated (csv) file with $n$ rows (samples) and $q$ columns (features), without rownames or colnames.
- `filename_label`: The filename of the labels. It must be a single-column file with $n$ rows (samples) and integer values, without rownames or colnames.
- `conf_type`: The confounder type, which is a string out of `conti` for continuous variable, `categ` for categorical variable, and `multi` for multiple variables.
- `filename_conf`: The filename of the confounders. It must be a comma-separated (csv) file with $n$ rows (samples) and $p$ columns (confounders), without rownames or colnames. For `conf_type` of `conti` and `categ`, it is a single-column file. For `conf_type` of `multi`, it is a multi-column file.
### Output:
- The training and validation process is stored in the folder `lightning_logs/cXVAE`, which can be inspected and visualized on TensorBoard.
- Multiple metrics on the test set are stored in the file `lightning_logs/cXVAE/version_0/results_performance.csv`, including the reconstruction error, dispersion score of the consensus clustering, Silhouette score, DB index, adjusted Rand index with true labels, normalized mutual information with true labels, adjusted Rand index with confounders, normalized mutual information with confounders. More details about these metrics can be found in our publication [2].
### Example
Here is an example of how to run `cXVAE_wrapper.py` on TCGA data with simulated confounders. Example datasets can be downloaded from https://drive.google.com/drive/folders/1vg_bg5hXDLRNkmAtIELrj3WsK1k8EjQU?usp=sharing. They are currently not available on GitHub due to the large file size. The required files are:

- `TCGA_mRNA_confounded.csv`: The first data view, with 2000 gene expression features for 2547 cancer patients.
- `TCGA_DNAm_confounded.csv`: The second data view, with 2000 DNA methylation features for 2547 cancer patients.
- `TCGA_confounder.csv`: The confounder data, simulated to be a categorical variable for 2547 cancer patients.
- `TCGA_cancerTypes.csv`: the label data, with 6 different cancer types for 2547 cancer patients.

To run the example, place these 4 files in the folder `Multi-Omics-Deconfounding-VAE` and run:
```
## cd Multi-Omics-Deconfounding-VAE
python  cXVAE_wrapper.py  path/to/Multi-Omics-Deconfounding-VAE/  TCGA_mRNA_confounded.csv  TCGA_DNAm_confounded.csv  TCGA_cancerTypes.csv  categ  TCGA_confounder.csv
```

&nbsp;
For the other models, users can run them with the scripts in the folder `scripts`, as follows:

- Vanilla XVAE: `XVAE/trainModel.py`
- cXVAE: `cXVAE/trainModel_cXVAE.py`
- XVAE with adversarial training: `adversarial_XVAE/trainModel_VAE_adversarial_multiclass.py` and `adversarial_XVAE/trainModel_VAE_adversarial_multinet.py`
- XVAE with regularized loss function: `XVAE_corrReg/trainModel_corrReg.py`
- XVAE with removal of latent features: `XVAE/trainModel_removeLatFeatures.py`


## References
> [1] Simidjievski N, Bodnar C, Tariq I, Scherer P, Andres Terre H, Shams Z, Jamnik M and Liò P (2019) Variational Autoencoders for Cancer Data Integration: Design Principles and Computational Practice. *Frontiers in Genetics*, 10:1205. doi: 10.3389/fgene.2019.01205 \
> [2] Zuqi Li, Sonja Katz, Edoardo Saccenti, David W Fardo, Peter Claes, Vitor A P Martins dos Santos, Kristel Van Steen, Gennady V Roshchupkin, Novel multi-omics deconfounding variational autoencoders can obtain meaningful disease subtyping, *Briefings in Bioinformatics*, Volume 25, Issue 6, November 2024, bbae512, doi: 10.1093/bib/bbae512 \
> [3] Ayse B Dincer, Joseph D Janizek, Su-In Lee, Adversarial deconfounding autoencoder for learning robust gene expression embeddings, Bioinformatics, Volume 36, Issue Supplement_2, December 2020, Pages i573–i582, doi: 10.1093/bioinformatics/btaa796 \
> [4] Mojtaba Bahrami, Malosree Maitra, Corina Nagy, Gustavo Turecki, Hamid R Rabiee, Yue Li, Deep feature extraction of single-cell transcriptomes by generative adversarial network, Bioinformatics, Volume 37, Issue 10, May 2021, Pages 1345–1351, doi: 10.1093/bioinformatics/btaa976 \
> [5] Xianjing Liu, Bo Li, Esther E. Bron, Wiro J. Niessen, Eppo B. Wolvius, and Gennady V. Roshchupkin. "Projection-wise disentangling for fair and interpretable representation learning: Application to 3d facial shape analysis." In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pp. 814-823. Springer International Publishing, 2021. doi: 10.1007/978-3-030-87240-3_78

 
